import os
import requests
from urllib.parse import urlparse, unquote

OUTPUT_DIR = "output/pdfs"

def clear_previous_downloads():
    """
    output/pdfs/ ãƒ•ã‚©ãƒ«ãƒ€å†…ã®å‰å›ã®å€‹åˆ¥PDFã‚’ã™ã¹ã¦å‰Šé™¤
    æ¬¡ã«æ–°ã—ã„è‡ªæ²»ä½“ã‚’å‡¦ç†ã™ã‚‹ã¨ãã«å¿…ãšå‘¼ã³å‡ºã™
    """
    if not os.path.exists(OUTPUT_DIR):
        return

    deleted_count = 0
    for filename in os.listdir(OUTPUT_DIR):
        file_path = os.path.join(OUTPUT_DIR, filename)
        try:
            if os.path.isfile(file_path) or os.path.islink(file_path):
                os.unlink(file_path)  # ãƒ•ã‚¡ã‚¤ãƒ«ã‚„ã‚·ãƒ³ãƒœãƒªãƒƒã‚¯ãƒªãƒ³ã‚¯ã‚’å‰Šé™¤
                deleted_count += 1
        except Exception as e:
            print(f"âš  å‰Šé™¤å¤±æ•—: {file_path} ({e})")

    if deleted_count > 0:
        print(f"ğŸ—‘ å‰å›ã®å€‹åˆ¥PDF {deleted_count} ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã™ã¹ã¦å‰Šé™¤ã—ã¾ã—ãŸ")
    else:
        print("ğŸ§¹ å‰å›ã®å€‹åˆ¥PDFã¯ã‚ã‚Šã¾ã›ã‚“ï¼ˆã‚¯ãƒªãƒ¼ãƒ³ãªçŠ¶æ…‹ï¼‰")


def download_pdfs(pdf_urls):
    """
    PDFã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ãƒ¡ã‚¤ãƒ³é–¢æ•°
    å‘¼ã³å‡ºã—å‰ã« clear_previous_downloads() ã‚’å®Ÿè¡Œã™ã‚‹ã“ã¨ã‚’æ¨å¥¨
    """
    # ãƒ•ã‚©ãƒ«ãƒ€ä½œæˆ
    os.makedirs(OUTPUT_DIR, exist_ok=True)

    # â˜…ã“ã“ãŒé‡è¦ï¼šæœ€åˆã«å‰å›ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã™ã¹ã¦å‰Šé™¤â˜…
    clear_previous_downloads()

    downloaded = []

    for url in pdf_urls:
        try:
            parsed = urlparse(url)
            filename = os.path.basename(parsed.path)

            # URLã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰è§£é™¤ï¼ˆæ—¥æœ¬èªãƒ•ã‚¡ã‚¤ãƒ«åå¯¾ç­–ï¼‰
            filename = unquote(filename)

            if not filename.lower().endswith(".pdf"):
                print(f"â­ PDFã§ãªã„ãŸã‚ã‚¹ã‚­ãƒƒãƒ—: {filename}")
                continue

            save_path = os.path.join(OUTPUT_DIR, filename)

            print(f"ğŸ“¥ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­: {filename}")

            r = requests.get(url, timeout=30, stream=True)
            r.raise_for_status()

            with open(save_path, "wb") as f:
                for chunk in r.iter_content(chunk_size=8192):
                    if chunk:
                        f.write(chunk)

            print(f"âœ“ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Œäº†: {filename}")
            downloaded.append(save_path)

        except Exception as e:
            print(f"âš  ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¤±æ•—: {url} ({e})")

    return downloaded