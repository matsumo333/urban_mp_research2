import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse, urldefrag
import time
import re

# --- ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰è¨­å®š ---
PRIMARY_GOAL_KEYWORDS = ["éƒ½å¸‚è¨ˆç”»ãƒã‚¹ã‚¿ãƒ¼ãƒ—ãƒ©ãƒ³", "éƒ½å¸‚ãƒã‚¹ã‚¿ãƒ¼ãƒ—ãƒ©ãƒ³", "éƒ½å¸‚ãƒã‚¹ã‚¿"]
SECONDARY_GOAL_KEYWORDS = ["ç·åˆè¨ˆç”»", "ç«‹åœ°é©æ­£åŒ–", "ã¾ã¡ã¥ãã‚Šè¨ˆç”»", "æ–½ç­–"]
PARENT_KEYWORDS = ["ã¾ã¡ã¥ãã‚Š", "éƒ½å¸‚è¨ˆç”»", "æ™¯è¦³"]
EXCLUDE_TEXT_KEYWORDS = ["ç§»è»¢ã—", "ç§»è»¢ã—ã¾ã—ãŸ", "é–‰é–"]

def search(start_url: str, max_depth: int = 5):
    domain = urlparse(start_url).netloc
    visited = set()
    to_crawl = []  # (url, depth, priority)
    
    primary_found = []
    secondary_found = []

    # â˜…è¿½åŠ ï¼šã‚µã‚¤ãƒˆãƒãƒƒãƒ—ã§ã€Œéƒ½å¸‚è¨ˆç”»ã€ãŒè¦‹ã¤ã‹ã£ãŸã‹ã©ã†ã‹ã®ãƒ•ãƒ©ã‚°
    urban_planning_in_sitemap = False

    start_url = start_url.rstrip("/") + "/"
    to_crawl.append((start_url, 0, 0))

    while to_crawl and (len(primary_found) + len(secondary_found)) < 40:
        to_crawl.sort(key=lambda x: x[2], reverse=True)
        current_url, depth, priority = to_crawl.pop(0)

        if depth > max_depth or current_url in visited:
            continue

        visited.add(current_url)
        print(f"  [æ¢ç´¢ä¸­] {current_url}")

        # â˜…è¿½åŠ ï¼šç¾åœ¨å‡¦ç†ä¸­ã®ãƒšãƒ¼ã‚¸ãŒã‚µã‚¤ãƒˆãƒãƒƒãƒ—ã‹ã©ã†ã‹ã‚’åˆ¤å®š
        is_sitemap_page = ("sitemap" in current_url.lower()) or ("ã‚µã‚¤ãƒˆãƒãƒƒãƒ—" in current_url)

        try:
            r = requests.get(current_url, timeout=15)
            r.encoding = r.apparent_encoding
            soup = BeautifulSoup(r.text, "html.parser")

            for a in soup.find_all("a", href=True):
                raw_text = a.get_text(strip=True)
                
                if any(ex in raw_text for ex in EXCLUDE_TEXT_KEYWORDS):
                    continue

                href = a["href"]
                full_url = urljoin(current_url, href)
                
                if any(k in raw_text for k in PRIMARY_GOAL_KEYWORDS) or "#toshimasu" in full_url.lower():
                    pass 
                else:
                    full_url = urldefrag(full_url)[0]

                if urlparse(full_url).netloc != domain:
                    continue

                new_priority = 0

                # 1. æœ¬å‘½ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
                if any(k in raw_text for k in PRIMARY_GOAL_KEYWORDS):
                    new_priority = 1000
                    if full_url not in [u[0] for u in primary_found]:
                        primary_found.append((full_url, raw_text))
                        print(f"    â­ æœ¬å‘½ç™ºè¦‹: {raw_text}")

                # 2. äºˆå‚™ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆç·åˆè¨ˆç”»ãªã©ï¼‰
                elif any(k in raw_text for k in SECONDARY_GOAL_KEYWORDS):
                    # â˜…è¿½åŠ ï¼šã‚µã‚¤ãƒˆãƒãƒƒãƒ—ã§ã€Œéƒ½å¸‚è¨ˆç”»ã€ãŒè¦‹ã¤ã‹ã£ã¦ã„ãŸã‚‰äºˆå‚™ã‚’å®Œå…¨ã«ç„¡è¦–
                    if urban_planning_in_sitemap:
                        continue  # ã‚¯ãƒ­ãƒ¼ãƒ«å€™è£œã«ã‚‚å…¥ã‚Œãªã„
                    new_priority = 100
                    if full_url not in [u[0] for u in secondary_found]:
                        secondary_found.append((full_url, raw_text))

                # 3. ã‚µã‚¤ãƒˆãƒãƒƒãƒ—ãƒ»è¦ªã‚«ãƒ†ã‚´ãƒª
                elif "sitemap" in full_url.lower() or "ã‚µã‚¤ãƒˆãƒãƒƒãƒ—" in raw_text:
                    new_priority = 800
                elif any(k in raw_text for k in PARENT_KEYWORDS):
                    new_priority = 500

                    # â˜…è¿½åŠ ï¼šã‚µã‚¤ãƒˆãƒãƒƒãƒ—ãƒšãƒ¼ã‚¸å†…ã§ã€Œéƒ½å¸‚è¨ˆç”»ã€ãŒã‚ã£ãŸã‚‰ãƒ•ãƒ©ã‚°ã‚’ç«‹ã¦ã‚‹
                    if is_sitemap_page and "éƒ½å¸‚è¨ˆç”»" in raw_text:
                        urban_planning_in_sitemap = True
                        print("    ğŸš© ã‚µã‚¤ãƒˆãƒãƒƒãƒ—å†…ã§ã€Œéƒ½å¸‚è¨ˆç”»ã€ç™ºè¦‹ â†’ äºˆå‚™ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®æ¢ç´¢ã‚’æŠ‘åˆ¶ã—ã¾ã™")

                # ã‚¯ãƒ­ãƒ¼ãƒ«å€™è£œã«è¿½åŠ 
                if new_priority > 0 and full_url not in visited:
                    if not any(url == full_url for url, d, p in to_crawl):
                        to_crawl.append((full_url, depth + 1, new_priority))

            time.sleep(0.5)
        except Exception as e:
            print(f"    ã‚¨ãƒ©ãƒ¼: {e}")
            continue

    # æœ€çµ‚ãƒªã‚¹ãƒˆï¼ˆå¾“æ¥é€šã‚Šï¼‰
    if primary_found:
        print("\nâœ… æœ¬å‘½ï¼ˆãƒã‚¹ã‚¿ãƒ¼ãƒ—ãƒ©ãƒ³ç­‰ï¼‰ãŒè¦‹ã¤ã‹ã£ãŸãŸã‚ã€äºˆå‚™ã¯é™¤å¤–ã—ã¾ã—ãŸã€‚")
        results = [u[0] for u in primary_found]
    else:
        print("\nâ„¹ æœ¬å‘½ãŒè¦‹ã¤ã‹ã‚‰ãªã‹ã£ãŸãŸã‚ã€äºˆå‚™ï¼ˆç·åˆè¨ˆç”»ç­‰ï¼‰ã‚’è¡¨ç¤ºã—ã¾ã™ã€‚")
        results = [u[0] for u in secondary_found]
            
    return results

if __name__ == "__main__":
    target = "https://www.info.city.tsu.mie.jp/www/sitemap/index.html"  # ä¾‹: ã‚µã‚¤ãƒˆãƒãƒƒãƒ—ã‹ã‚‰ã‚¹ã‚¿ãƒ¼ãƒˆã™ã‚‹å ´åˆ
    # target = "https://www.city.example.jp/"  # ãƒˆãƒƒãƒ—ãƒšãƒ¼ã‚¸ã‹ã‚‰ã‚¹ã‚¿ãƒ¼ãƒˆã™ã‚‹å ´åˆã‚‚å¯
    final_urls = search(target)
    
    print("\n--- æœ€çµ‚çµæœ ---")
    for i, url in enumerate(final_urls, 1):
        print(f"{i}: {url}")