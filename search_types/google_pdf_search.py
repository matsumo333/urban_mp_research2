import time
import urllib.parse
import re
import csv
import os
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By

SEARCH_WORD = "éƒ½å¸‚è¨ˆç”» ãƒã‚¹ã‚¿ãƒ¼ãƒ—ãƒ©ãƒ³"

# â˜… ä¿å­˜å…ˆCSVãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ï¼ˆå¿…è¦ã«å¿œã˜ã¦å¤‰æ›´ï¼‰
LINKS_CSV_PATH = "output/links.csv"  # æ—¢å­˜ã®ã‚¢ãƒ—ãƒªã¨åˆã‚ã›ã‚‹å ´åˆ
# ã¾ãŸã¯ "links_from_google.csv" ãªã©å°‚ç”¨åã§ã‚‚OK

def clean_title(raw_text: str) -> str:
    lines = [l.strip() for l in raw_text.splitlines() if l.strip()]
    cleaned = []
    for line in lines:
        if re.search(r"https?://|www\.|â€º", line):
            continue
        cleaned.append(line)
    return cleaned[0] if cleaned else "åç§°ä¸æ˜"


def save_links_to_csv(links, csv_path=LINKS_CSV_PATH):
    """
    ãƒªãƒ³ã‚¯ãƒªã‚¹ãƒˆã‚’CSVã«ä¿å­˜
    links: [("title1", "url1"), ("title2", "url2"), ...]
    """
    os.makedirs(os.path.dirname(csv_path), exist_ok=True) if os.path.dirname(csv_path) else None

    with open(csv_path, "w", encoding="utf-8-sig", newline="") as f:
        writer = csv.writer(f)
        writer.writerow(["title", "url"])  # ãƒ˜ãƒƒãƒ€ãƒ¼
        for title, url in links:
            writer.writerow([title, url])

    print(f"ğŸ’¾ ãƒªãƒ³ã‚¯ã‚’CSVã«ä¿å­˜ã—ã¾ã—ãŸ: {csv_path}ï¼ˆ{len(links)}ä»¶ï¼‰")


def search(city_name: str, max_pages: int = 1):
    """
    Googleã§åºƒãé–¢é€£ãƒšãƒ¼ã‚¸ã‚’æ¤œç´¢ â†’ çµæœã‚’CSVã«ä¿å­˜ã—ã¦è¿”ã™
    """
    options = Options()
    options.add_argument("--window-size=1200,900")

    driver = webdriver.Chrome(options=options)
    results = []  # (title, url) ã®ãƒªã‚¹ãƒˆ

    try:
        # filetype:pdf ã‚’å¤–ã—ã¦åºƒãæ¤œç´¢
        query = f'{city_name} {SEARCH_WORD} site:.go.jp OR site:.lg.jp'
        encoded = urllib.parse.quote(query)
        search_url = f"https://www.google.com/search?q={encoded}&num=30"

        print(f"ğŸ” GoogleåºƒåŸŸæ¤œç´¢ä¸­: {query}")
        print("   ï¼ˆãƒ­ãƒœãƒƒãƒˆèªè¨¼ãŒå‡ºãŸã‚‰æ‰‹å‹•ã§è§£é™¤ â†’ 30ç§’å¾…æ©Ÿï¼‰")

        driver.get(search_url)
        time.sleep(30)  # æ‰‹å‹•å¯¾å¿œæ™‚é–“

        for a in driver.find_elements(By.CSS_SELECTOR, "a"):
            href = a.get_attribute("href")
            if not href or href.startswith(("javascript:", "/")) or "google" in href:
                continue

            raw_title = a.text.strip()
            title = clean_title(raw_title)
            if not title or title == "åç§°ä¸æ˜":
                try:
                    h3 = a.find_element(By.XPATH, ".//h3")
                    title = h3.text.strip()
                except:
                    pass
                if not title:
                    continue

            results.append((title, href))

        # é‡è¤‡é™¤å»
        seen = set()
        unique = []
        for item in results:
            if item[1] not in seen:
                unique.append(item)
                seen.add(item[1])

        # â˜…â˜…â˜… CSVã«ä¿å­˜ â˜…â˜…â˜…
        save_links_to_csv(unique)

        print(f"âœ… æ¤œç´¢å®Œäº†ï¼ç™ºè¦‹ãƒªãƒ³ã‚¯æ•°: {len(unique)}ä»¶")
        return unique

    except Exception as e:
        print(f"âš  æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")
        return []
    finally:
        driver.quit()


# ãƒ†ã‚¹ãƒˆç”¨ï¼ˆç›´æ¥å®Ÿè¡Œæ™‚ï¼‰
if __name__ == "__main__":
    city = input("å¸‚ç”ºæ‘åã‚’å…¥åŠ›ï¼ˆä¾‹: æ¨ªæµœå¸‚ï¼‰: ").strip()
    if city:
        search(city)