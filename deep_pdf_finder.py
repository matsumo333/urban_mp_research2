import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse, unquote
import os

# =========================================
# è¨­å®š
# =========================================
KEYWORDS = ["éƒ½å¸‚", "ãƒã‚¹"]
PDF_DIR = "output/pdfs"


# =========================================
# PDFãƒ•ã‚¡ã‚¤ãƒ«ã ã‘å‰Šé™¤ï¼ˆãƒ•ã‚©ãƒ«ãƒ€ã¯æ®‹ã™ï¼‰
# =========================================
def clear_pdf_files():
    if not os.path.exists(PDF_DIR):
        return

    for filename in os.listdir(PDF_DIR):
        if filename.lower().endswith(".pdf"):
            try:
                os.remove(os.path.join(PDF_DIR, filename))
            except Exception as e:
                print(f"âš ï¸ å‰Šé™¤å¤±æ•—: {filename} ({e})")


# =========================================
# PDFæ¢ç´¢ãƒ»ä¿å­˜ãƒ¡ã‚¤ãƒ³é–¢æ•°
# =========================================
def find_pdfs_recursively(start_url, city, max_depth=4):
    # â˜… å®Ÿè¡Œæ™‚ã«éå»PDFã‚’å…¨å‰Šé™¤ï¼ˆâ‘¡æ–¹å¼ï¼‰
    clear_pdf_files()

    visited = set()
    results = []

    os.makedirs(PDF_DIR, exist_ok=True)

    # -------------------------------------
    # PDFãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
    # -------------------------------------
    def download_pdf(pdf_url):
        filename = os.path.basename(urlparse(pdf_url).path)
        filename = unquote(filename) or "document.pdf"

        safe_city = city.replace(" ", "")
        save_name = f"{safe_city}_{filename}"
        save_path = os.path.join(PDF_DIR, save_name)

        if os.path.exists(save_path):
            return save_path, "SKIP_EXISTS"

        try:
            r = requests.get(pdf_url, timeout=30, stream=True)
            r.raise_for_status()

            content_type = r.headers.get("Content-Type", "").lower()
            if "application/pdf" not in content_type:
                return None, "NOT_A_PDF_CONTENT"

            with open(save_path, "wb") as f:
                for chunk in r.iter_content(chunk_size=8192):
                    f.write(chunk)

            return save_path, "OK"

        except requests.HTTPError:
            return None, f"HTTP_{r.status_code}"
        except Exception as e:
            return None, f"ERROR_{type(e).__name__}"

    # -------------------------------------
    # å†å¸°ã‚¯ãƒ­ãƒ¼ãƒ«
    # -------------------------------------
    def crawl(url, depth):
        if depth > max_depth:
            return
        if url in visited:
            return

        visited.add(url)
        print(f"ğŸ” æ¢ç´¢ä¸­ (depth={depth}): {url}")

        try:
            r = requests.get(url, timeout=15)
            r.raise_for_status()
            r.encoding = r.apparent_encoding
        except Exception:
            return

        soup = BeautifulSoup(r.text, "html.parser")

        # â‘  PDFãƒªãƒ³ã‚¯æ¢ç´¢
        for a in soup.select("a[href*='.pdf']"):
            href = a.get("href")
            if not href:
                continue

            title = a.get_text(strip=True)
            pdf_url = urljoin(url, href)

            local_path, status = download_pdf(pdf_url)

            print(f"  {'âœ…' if status == 'OK' else 'âš ï¸'} {status}: {pdf_url}")

            results.append(
                {
                    "city": city,
                    "title": title if title else "PDFï¼ˆåç§°ä¸æ˜ï¼‰",
                    "type": "PDF",
                    "url": pdf_url,
                    "local_path": local_path or "",
                    "source": url,
                    "depth": depth,
                    "status": status,
                }
            )

        # â‘¡ æ¬¡ã®HTMLãƒªãƒ³ã‚¯æ¢ç´¢
        for a in soup.select("a[href]"):
            text = a.get_text(separator="", strip=True)
            href = a.get("href")

            if not text or not href:
                continue

            if not any(k in text for k in KEYWORDS):
                continue

            next_url = urljoin(url, href)

            if urlparse(next_url).netloc != urlparse(start_url).netloc:
                continue

            crawl(next_url, depth + 1)

    # å®Ÿè¡Œ
    crawl(start_url, 0)
    return results
