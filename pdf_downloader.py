import os
import requests
from urllib.parse import urlparse, unquote

OUTPUT_DIR = "output/pdfs"

def download_pdfs(pdf_urls):
    os.makedirs(OUTPUT_DIR, exist_ok=True)

    downloaded = []

    for url in pdf_urls:
        try:
            parsed = urlparse(url)
            filename = os.path.basename(parsed.path)

            # URLã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰è§£é™¤ï¼ˆæ—¥æœ¬èªãƒ•ã‚¡ã‚¤ãƒ«åå¯¾ç­–ï¼‰
            filename = unquote(filename)

            if not filename.lower().endswith(".pdf"):
                continue

            save_path = os.path.join(OUTPUT_DIR, filename)

            # æ—¢ã«å­˜åœ¨ã™ã‚‹å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
            if os.path.exists(save_path):
                print(f"â­ æ—¢ã«å­˜åœ¨: {filename}")
                continue

            print(f"ğŸ“¥ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­: {filename}")

            r = requests.get(url, timeout=20, stream=True)
            r.raise_for_status()

            with open(save_path, "wb") as f:
                for chunk in r.iter_content(chunk_size=8192):
                    if chunk:
                        f.write(chunk)

            downloaded.append(save_path)

        except Exception as e:
            print(f"âš  ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¤±æ•—: {url} ({e})")

    return downloaded
