# main.py

from search_strategy_detector import detect_search_strategy
from municipality_detector import detect_municipality_name

from search_types.google_cse import search as google_cse_search
from search_types.internal_search import search as internal_search
from search_types.fallback import search as fallback_search
from search_types.topical_entry import search as topical_entry_search

from link_extractor import extract_links, save_links_csv, load_links_csv
from deep_pdf_finder import find_pdfs_recursively
from result_collector import save_results
from pdf_selector_gui import show_pdf_selector


# ==========================================
# è¨­å®š
# ==========================================
MAX_PAGES = 5
LINKS_CSV = r"C:\Users\matsu\Desktop\python\urban_mp_research\output\links.csv"


def main():
    # ==============================
    # â‘  è‡ªæ²»ä½“ãƒˆãƒƒãƒ—URLå…¥åŠ›
    # ==============================
    url = input("è‡ªæ²»ä½“ãƒˆãƒƒãƒ—ãƒšãƒ¼ã‚¸URLã‚’å…¥åŠ›ã—ã¦ãã ã•ã„: ").strip()
    if not url:
        print("âŒ URLãŒå…¥åŠ›ã•ã‚Œã¦ã„ã¾ã›ã‚“")
        return

    # ==============================
    # â‘¡ è‡ªæ²»ä½“åã‚’è‡ªå‹•æ¤œå‡º
    # ==============================
    city = detect_municipality_name(url)
    print(f"\nğŸ™ è‡ªå‹•æ¤œå‡ºã•ã‚ŒãŸè‡ªæ²»ä½“å: {city}")

    # ==============================
    # â‘¢ æ¤œç´¢æ–¹å¼ã‚’è‡ªå‹•åˆ¤åˆ¥
    # ==============================
    strategy = detect_search_strategy(url)
    print(f"ğŸ”§ æ¤œå‡ºã•ã‚ŒãŸæ¤œç´¢æ–¹å¼: {strategy}\n")

    # ==============================
    # â‘£ æ¤œç´¢å®Ÿè¡Œ
    # ==============================
    html = ""
    entry_urls = []

    try:
        if strategy == "google_cse":
            html = google_cse_search(start_url=url, max_pages=MAX_PAGES)

        elif strategy == "internal_search":
            html = internal_search(start_url=url, max_pages=MAX_PAGES)

        elif strategy == "topical_entry":
            # ğŸ”‘ ç¥æˆ¸å¸‚ãƒ»æ¨ªæµœå¸‚ã‚¿ã‚¤ãƒ—
            print("ğŸ” ãƒˆãƒƒãƒ—ãƒšãƒ¼ã‚¸å°ç·šå‹æ¤œç´¢ã‚’ä½¿ç”¨ã—ã¾ã™")
            entry_urls = topical_entry_search(start_url=url)

        else:
            html = fallback_search(start_url=url)

    except Exception as e:
        print(f"âŒ æ¤œç´¢å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        return

    # ==============================
    # â‘¤ ãƒˆãƒƒãƒ—ãƒšãƒ¼ã‚¸å°ç·šå‹ã®å ´åˆ
    # ==============================
    if strategy == "topical_entry":
        if not entry_urls:
            print("âŒ éƒ½å¸‚è¨ˆç”»ãƒ»ã¾ã¡ã¥ãã‚Šå°ç·šãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
            return

        records = []

        for entry in entry_urls:
            print(f"ğŸ” å°ç·šæ¢ç´¢é–‹å§‹: {entry}")
            try:
                found = find_pdfs_recursively(
                    start_url=entry,
                    city=city,
                    max_depth=5
                )
                records.extend(found)
            except Exception as e:
                print(f"âš ï¸ æ¢ç´¢ã‚¨ãƒ©ãƒ¼: {e}")

        if not records:
            print("âŒ PDFã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")
            return

        save_results(records)
        show_pdf_selector()
        print("\nğŸŠ ã™ã¹ã¦ã®å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸï¼")
        return

    # ==============================
    # â‘¥ é€šå¸¸æ¤œç´¢ï¼šHTMLè§£æ
    # ==============================
    if not html:
        print("âŒ æ¤œç´¢çµæœHTMLã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")
        return

    links = extract_links(html, base_url=url)

    if not links:
        print("âŒ é–¢é€£ãƒªãƒ³ã‚¯ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
        return

    # ä¸­é–“CSVã¯ä¿å­˜ã®ã¿ï¼ˆå°†æ¥å¾©æ´»ç”¨ï¼‰
    save_links_csv(links, LINKS_CSV)

    # ==============================
    # â‘¦ é‡è¤‡é™¤å»ï¼ˆURLåŸºæº–ï¼‰
    # ==============================
    seen = set()
    unique_links = []
    for title, link in links:
        if link not in seen:
            seen.add(link)
            unique_links.append((title, link))

    print(f"\nâœ” æ·±æ˜ã‚Šå¯¾è±¡ãƒªãƒ³ã‚¯æ•°: {len(unique_links)} ä»¶\n")

    # ==============================
    # â‘§ æ·±æ˜ã‚Šæ¢ç´¢ï¼ˆPDFåé›†ï¼‰
    # ==============================
    records = []

    for idx, (title, link) in enumerate(unique_links, 1):
        print(f"â–¶ å‡¦ç† {idx}/{len(unique_links)}: {title}")

        try:
            if link.lower().endswith(".pdf"):
                found = find_pdfs_recursively(
                    start_url=link,
                    city=city,
                    max_depth=1
                )
            else:
                found = find_pdfs_recursively(
                    start_url=link,
                    city=city,
                    max_depth=4
                )

            records.extend(found)

        except Exception as e:
            print(f"âš ï¸ æ·±æ˜ã‚Šä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")

    if not records:
        print("\nâŒ PDFã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")
        return

    # ==============================
    # â‘¨ æœ€çµ‚CSVä¿å­˜
    # ==============================
    save_results(records)

    # ==============================
    # â‘© PDFé¸æŠGUIèµ·å‹•
    # ==============================
    print("\nğŸ–¥ PDFé¸æŠç”»é¢ã‚’èµ·å‹•ã—ã¾ã™...")
    try:
        show_pdf_selector()
    except Exception as e:
        print(f"âš ï¸ GUIèµ·å‹•ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")

    print("\nğŸŠ ã™ã¹ã¦ã®å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸï¼")


if __name__ == "__main__":
    main()
